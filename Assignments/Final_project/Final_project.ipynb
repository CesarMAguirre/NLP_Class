{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"},{"sourceId":8263220,"sourceType":"datasetVersion","datasetId":4904844}],"dockerImageVersionId":30699,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nimport re\nimport json\nimport torch\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm.notebook import tqdm\nfrom collections import Counter\nfrom torch.utils.data import Dataset, DataLoader\nfrom concurrent.futures import ProcessPoolExecutor\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\n\n# Path and configuration settings\ntest_data_file = \"/kaggle/input/pii-detection-removal-from-educational-data/test.json\"\nweights_path = \"/kaggle/input/piid-modelweights/\"\nchunk_size = 5_000\ntokenizer_stride = 32\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Enhanced model parameters with weights\nmodel_params = [\n    {\"path\":\"model_387\", \"max_tokens\":512, \"weight\": 0.8, \"specialization\": \"precision\"},\n    {\"path\":\"model_539\", \"max_tokens\":1024, \"weight\": 1.0, \"specialization\": \"balanced\"},\n    {\"path\":\"model_543\", \"max_tokens\":1024, \"weight\": 1.2, \"specialization\": \"balanced\"},\n    {\"path\":\"model_560\", \"max_tokens\":2048, \"weight\": 1.5, \"specialization\": \"recall\"},\n    {\"path\":\"model_563\", \"max_tokens\":2048, \"weight\": 1.5, \"specialization\": \"recall\"},\n    {\"path\":\"model_572\", \"max_tokens\":1024, \"weight\": 1.0, \"specialization\": \"balanced\"},\n]\n\n# Global token cache for efficient processing\ntoken_prediction_cache ={}\n\n# Entity mapping for NER tagging\nentity_mapping = {\n    0: \"O\",\n    1: \"B-NAME_STUDENT\",\n    2: \"B-URL_PERSONAL\",\n    3: \"B-ID_NUM\",\n    4: \"B-EMAIL\",\n    5: \"B-STREET_ADDRESS\",\n    6: \"B-PHONE_NUM\",\n    7: \"B-USERNAME\",\n    101: \"I-NAME_STUDENT\",\n    102: \"I-URL_PERSONAL\",\n    103: \"I-ID_NUM\",\n    104: \"I-EMAIL\",\n    105: \"I-STREET_ADDRESS\",\n    106: \"I-PHONE_NUM\",\n    107: \"I-USERNAME\",\n}\n# Regex for entity detection\nregex_patterns = {\n    \"email\": r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\",\n    \"phone\": r\"^\\+?[\\d\\-\\(\\)\\s]{7,15}$\",\n    \"id_number\": r\"^[A-Z0-9]{5,12}$|^\\d{3}-\\d{2}-\\d{4}$\",\n    \"username\": r\"^@[a-zA-Z0-9_]{3,15}$|^[a-zA-Z][a-zA-Z0-9_]{2,15}$\",\n    \"url\": r\"^(https?:\\/\\/)?(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)$\"\n}","metadata":{"execution":{"iopub.status.busy":"2025-05-12T01:18:59.219913Z","iopub.execute_input":"2025-05-12T01:18:59.220220Z","iopub.status.idle":"2025-05-12T01:19:05.971681Z","shell.execute_reply.started":"2025-05-12T01:18:59.220184Z","shell.execute_reply":"2025-05-12T01:19:05.970740Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Let's define some functions \ndef chunk(l, n): \n    \"\"\"Split list into chunks of size n\"\"\"\n    for i in range(0, len(l), n):  \n        yield l[i:i + n] \n\ndef decode_targets(targets: list, doc_ids: list) -> list:\n    \"\"\"Convert numeric targets to BIO tagging format\"\"\"\n    df = pd.DataFrame({\n        \"target\": targets,\n        \"document\": doc_ids\n    })\n    # Mark consecutive tokens of same entity as Inside (I-)\n    df[\"prev_target\"] = df.groupby(\"document\")[\"target\"].shift(1).values\n    cond = (df[\"prev_target\"] == df[\"target\"]) & (~df[\"prev_target\"].isnull())\n    df[\"target\"] += 100*cond.astype(int)\n    \n    # Map numeric targets to NER tags\n    df[\"target\"] = df[\"target\"].map(entity_mapping)\n    \n    return df[\"target\"].values.tolist()\n\ndef extract_character_features(tokens):\n    \"\"\"Extract character-level features from tokens\"\"\"\n    features = []\n    for token in tokens:\n        if not token:  # Handle empty tokens\n            features.append({\n                \"length\": 0,\n                \"has_digits\": False,\n                \"has_uppercase\": False,\n                \"has_lowercase\": False,\n                \"has_special\": False,\n                \"digit_ratio\": 0,\n                \"uppercase_ratio\": 0,\n                \"starts_uppercase\": False,\n                \"has_email_char\": False,\n                \"has_url_char\": False\n            })\n            continue\n            \n        features.append({\n            \"length\": len(token),\n            \"has_digits\": any(c.isdigit() for c in token),\n            \"has_uppercase\": any(c.isupper() for c in token),\n            \"has_lowercase\": any(c.islower() for c in token),\n            \"has_special\": any(not c.isalnum() for c in token),\n            \"digit_ratio\": sum(c.isdigit() for c in token) / max(len(token), 1),\n            \"uppercase_ratio\": sum(c.isupper() for c in token) / max(len(token), 1),\n            \"starts_uppercase\": token[0].isupper() if token else False,\n            \"has_email_char\": \"@\" in token,\n            \"has_url_char\": \"/\" in token or \".\" in token\n        })\n    return features","metadata":{"execution":{"iopub.status.busy":"2025-05-12T01:19:05.973043Z","iopub.execute_input":"2025-05-12T01:19:05.973425Z","iopub.status.idle":"2025-05-12T01:19:05.984362Z","shell.execute_reply.started":"2025-05-12T01:19:05.973403Z","shell.execute_reply":"2025-05-12T01:19:05.983377Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Data Processing \n\nclass ListDataset(Dataset):\n    \"\"\"Simple dataset from a list of items\"\"\"\n    def __init__(self, data_list):\n        self.data = data_list\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        return self.data[index]\n\ndef tokenize_and_batch_record(record, tokenizer, max_n_tokens, stride):\n    \"\"\"Process a single record for transformer input\"\"\"\n    max_len = min(tokenizer.model_max_length, max_n_tokens)\n    tokenized_inputs = tokenizer(\n        record[\"tokens\"], \n        return_offsets_mapping=False,\n        verbose=False, \n        is_split_into_words=True, \n        add_special_tokens=True,\n        max_length=max_len, \n        stride=stride, \n        truncation=True, \n        return_overflowing_tokens=True\n    )\n\n    # Extract character-level features\n    char_features = extract_character_features(record[\"tokens\"])\n    \n    # Create a targets map if labels exist\n    if \"labels\" in record:\n        # Note: encode_targets function is assumed to be defined elsewhere\n        targets_map = {i:v for i,v in enumerate(encode_targets(record[\"labels\"]))}\n    else:\n        targets_map = {}\n\n    # Create batches from tokenized inputs\n    batch = [{\n        \"input_ids\": tokenized_inputs[\"input_ids\"][i],\n        \"attention_mask\": tokenized_inputs[\"attention_mask\"][i],\n        \"word_ids\": [-100 if x is None else x for x in tokenized_inputs.word_ids(i)],\n        \"targets\": [targets_map.get(x, -100) for x in tokenized_inputs.word_ids(i)],\n        \"document\": [record[\"document\"]] * len(tokenized_inputs[\"input_ids\"][i]),\n        # Add token texts for rule-based processing\n        \"tokens\": [record[\"tokens\"][x] if x is not None and x < len(record[\"tokens\"]) else \"\" \n                  for x in tokenized_inputs.word_ids(i)],\n    } for i in range(len(tokenized_inputs[\"input_ids\"]))]\n\n    return batch\n\ndef tokenize_and_batch(sample, tokenizer, max_n_tokens, stride):\n    \"\"\"Process a batch of records with progress tracking\"\"\"\n    tokenized_sample = [tokenize_and_batch_record(rec, tokenizer, max_n_tokens, stride) for rec in tqdm(sample)]\n    tokenized_sample = [x for xs in tokenized_sample for x in xs]\n    return tokenized_sample\n\ndef collate_fn(batch):\n    \"\"\"Combine individual samples into batches with padding\"\"\"\n    keys = batch[0].keys()\n    # Special handling for non-tensor data\n    non_tensor_keys = [\"tokens\"]\n    tensor_keys = [k for k in keys if k not in non_tensor_keys]\n    \n    # Process tensor data\n    seq = {k:[torch.tensor(x[k]) for x in batch] for k in tensor_keys}\n    result = {k:torch.nn.utils.rnn.pad_sequence(v, True, 0) for k,v in seq.items()}\n    \n    # Process non-tensor data\n    for k in non_tensor_keys:\n        result[k] = [item for x in batch for item in x[k]]\n        \n    return result\n    \ndef create_dataloader(test_data, max_n_tokens, tokenizer_stride):\n    \"\"\"Create a PyTorch DataLoader\"\"\"\n    tokenizer = AutoTokenizer.from_pretrained(weights_path + \"tokenizer\")\n    tokenized_data = tokenize_and_batch(test_data, tokenizer, max_n_tokens, tokenizer_stride)\n    return DataLoader(\n        ListDataset(tokenized_data),\n        batch_size = 4,\n        num_workers = 2,\n        pin_memory = True,\n        shuffle = False,\n        collate_fn = collate_fn,\n    )","metadata":{"execution":{"iopub.status.busy":"2025-05-12T01:19:05.986202Z","iopub.execute_input":"2025-05-12T01:19:05.986450Z","iopub.status.idle":"2025-05-12T01:19:06.001556Z","shell.execute_reply.started":"2025-05-12T01:19:05.986431Z","shell.execute_reply":"2025-05-12T01:19:06.000837Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Model Definition\nclass PiiDetectionModel(torch.nn.Module):\n    \"\"\"Enhanced wrapper around pre-trained token classification model\"\"\"\n    def __init__(self, load_path, specialization=None):\n        super().__init__()\n        self.backbone = AutoModelForTokenClassification.from_pretrained(load_path)\n        self.specialization = specialization\n        \n    def forward(self, d):\n        \"\"\"Run inference through the model\"\"\"\n        preds = self.backbone(\n            input_ids=d[\"input_ids\"], \n            attention_mask=d[\"attention_mask\"]\n        )\n        \n        # Apply specialization adjustments if needed\n        logits = preds.logits\n        if self.specialization == \"precision\":\n            # Slightly reduce probability of positive predictions to favor precision\n            logits[:, :, 1:] -= 0.2\n        elif self.specialization == \"recall\":\n            # Slightly boost probability of positive predictions to favor recall\n            logits[:, :, 1:] += 0.1\n            \n        return {\"logits\": logits}\n\ndef add_confidence_metrics(probs):\n    \"\"\"Calculate confidence metrics for predictions\"\"\"\n    # Calculate entropy (lower means more confident)\n    epsilon = 1e-10  # To avoid log(0)\n    entropy = -np.sum(probs * np.log(probs + epsilon), axis=1)\n    \n    # Calculate margin (difference between top two probabilities)\n    sorted_probs = np.sort(probs, axis=1)\n    margin = sorted_probs[:, -1] - sorted_probs[:, -2]\n    \n    return entropy, margin\n\ndef results_to_df(predictions, data, model_name):\n    \"\"\"Convert model outputs to pandas DataFrame\"\"\"\n    # Apply softmax to get class probabilities\n    probs = torch.nn.functional.softmax(predictions[\"logits\"], -1)\n    probs = probs.flatten(0,1).cpu().numpy()\n    \n    # Calculate confidence metrics\n    entropy, margin = add_confidence_metrics(probs)\n    \n    # Create DataFrame with probabilities\n    probs_df = pd.DataFrame(probs, columns=[f\"prob_{i}\" for i in range(8)])\n    \n    # Create DataFrame with metadata\n    res = pd.DataFrame({\n        \"document\": data[\"document\"].cpu().flatten(),\n        \"word_ids\": data[\"word_ids\"].cpu().flatten(),\n        \"targets\": data[\"targets\"].cpu().flatten(),\n        \"entropy\": entropy,\n        \"margin\": margin,\n        \"model_name\": model_name\n    })\n    \n    # Combine metadata and probabilities\n    res = pd.concat([res, probs_df], axis=1)\n    return res\n\ndef inference(model_cfg, test_data):\n    \"\"\"Run inference with a single model configuration\"\"\"\n    # Create model instance\n    model = PiiDetectionModel(\n        f\"{weights_path}/{model_cfg['path']}\", \n        specialization=model_cfg.get('specialization', None)\n    )\n    model.to(device)\n    model.eval()\n    \n    # Create data loader\n    dl = create_dataloader(test_data, model_cfg['max_tokens'], tokenizer_stride)\n\n    # Run inference\n    res_df = pd.DataFrame()\n    with torch.no_grad():\n        for data in dl:\n            # Move data to device\n            for k in data.keys():\n                if isinstance(data[k], torch.Tensor):\n                    data[k] = data[k].to(device)\n            \n            # Get predictions\n            preds = model(data)\n            \n            # Convert to DataFrame and add to results\n            batch_df = results_to_df(preds, data, model_cfg['path'])\n            res_df = pd.concat([res_df, batch_df])\n    \n    return res_df","metadata":{"execution":{"iopub.status.busy":"2025-05-12T01:19:06.002531Z","iopub.execute_input":"2025-05-12T01:19:06.002802Z","iopub.status.idle":"2025-05-12T01:19:06.018115Z","shell.execute_reply.started":"2025-05-12T01:19:06.002782Z","shell.execute_reply":"2025-05-12T01:19:06.017278Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Post processing\ndef weighted_aggregation(preds_df, model_params):\n    \"\"\"Aggregate predictions with weighted average\"\"\"\n    # Create weight lookup\n    weight_dict = {cfg[\"path\"]: cfg[\"weight\"] for cfg in model_params}\n    total_weight = sum(weight_dict.values())\n    \n    # Calculate weighted probabilities\n    for i in range(8):\n        preds_df[f\"weighted_prob_{i}\"] = preds_df[f\"prob_{i}\"] * preds_df[\"model_name\"].map(weight_dict)\n    \n    # Group by document and word_ids, and calculate weighted average\n    result = preds_df.groupby([\"document\", \"word_ids\"]).agg(\n        **{f\"prob_{i}\":(f'weighted_prob_{i}', 'sum') for i in range(8)},\n        entropy=(\"entropy\", \"mean\"),\n        margin=(\"margin\", \"mean\")\n    )\n    \n    # Normalize by total weight\n    for i in range(8):\n        result[f\"prob_{i}\"] = result[f\"prob_{i}\"] / total_weight\n    \n    return result.reset_index()\n\ndef apply_regex_rules(test_chunk_df):\n    \"\"\"Apply regex-based rules to improve detection\"\"\"\n    df = test_chunk_df.copy()\n    \n    # Apply email regex\n    email_mask = df[\"tokens\"].str.match(regex_patterns[\"email\"], na=False)\n    df.loc[email_mask, \"preds\"] = 4  # Email class\n    \n    # Apply phone regex\n    phone_mask = df[\"tokens\"].str.match(regex_patterns[\"phone\"], na=False)\n    df.loc[phone_mask, \"preds\"] = 6  # Phone class\n    \n    # Apply ID regex\n    id_mask = df[\"tokens\"].str.match(regex_patterns[\"id_number\"], na=False)\n    df.loc[id_mask, \"preds\"] = 3  # ID class\n    \n    # Apply username regex\n    username_mask = df[\"tokens\"].str.match(regex_patterns[\"username\"], na=False)\n    df.loc[username_mask, \"preds\"] = 7  # Username class\n    \n    # Apply URL regex\n    url_mask = df[\"tokens\"].str.match(regex_patterns[\"url\"], na=False)\n    df.loc[url_mask, \"preds\"] = 2  # URL class\n    \n    return df\n\ndef context_aware_processing(df):\n    \"\"\"Apply rules that consider token context\"\"\"\n    result = df.copy()\n    \n    # Create columns for previous and next tokens\n    result[\"prev_token\"] = result.groupby(\"document\")[\"tokens\"].shift(1)\n    result[\"next_token\"] = result.groupby(\"document\")[\"tokens\"].shift(-1)\n    \n    # Rule 1: Improve name detection with common name parts\n    common_titles = [\"Mr\", \"Mrs\", \"Ms\", \"Dr\", \"Prof\"]\n    is_after_title = result[\"prev_token\"].isin(common_titles)\n    is_capitalized = result[\"tokens\"].str.istitle()\n    likely_name = is_after_title & is_capitalized & (result[\"preds\"] == 0)\n    result.loc[likely_name, \"preds\"] = 1  # Mark as name\n    \n    # Rule 2: Improve address detection\n    address_indicators = [\"St\", \"Ave\", \"Rd\", \"Blvd\", \"Ln\", \"Dr\", \"Cir\", \"Apt\"]\n    preceded_by_number = result[\"prev_token\"].str.match(r\"^\\d+$\", na=False)\n    followed_by_address_indicator = result[\"next_token\"].isin(address_indicators)\n    likely_street = preceded_by_number & followed_by_address_indicator\n    result.loc[likely_street, \"preds\"] = 5  # Mark as address\n    \n    # Rule 3: Mark newlines as address components\n    result.loc[result[\"tokens\"] == \"\\n\", \"preds\"] = 5\n    \n    # Rule 4: Clean up predictions - names should start with capital letter\n    result.loc[(result[\"preds\"] == 1) & (~result[\"tokens\"].str.istitle()), \"preds\"] = 0\n    \n    # Rule 5: Consecutive tokens with same entity should be marked consistently\n    # Propagate entity labels to neighboring tokens if they look like part of the same entity\n    for entity_id in [1, 2, 3, 4, 5, 6, 7]:  # All entity types\n        entity_mask = result[\"preds\"] == entity_id\n        \n        # Forward propagation\n        for _ in range(2):  # Apply twice to handle longer entities\n            next_entity_mask = result.groupby(\"document\")[\"preds\"].shift(-1) == entity_id\n            propagate_forward = (~entity_mask) & next_entity_mask & result[\"tokens\"].notna()\n            result.loc[propagate_forward, \"preds\"] = entity_id + 100  # Mark as I-entity\n        \n        # Backward propagation\n        for _ in range(2):\n            prev_entity_mask = result.groupby(\"document\")[\"preds\"].shift(1) == entity_id\n            propagate_backward = (~entity_mask) & prev_entity_mask & result[\"tokens\"].notna()\n            result.loc[propagate_backward, \"preds\"] = entity_id + 100  # Mark as I-entity\n    \n    return result\n\ndef token_consistency_check(df):\n    \"\"\"Apply consistency checks across documents\"\"\"\n    # Create a mapping of commonly detected tokens to their most frequent entity\n    token_entity_counter = df.groupby(\"tokens\")[\"preds\"].apply(lambda x: Counter(x).most_common(1)[0] if len(x) > 0 else (0, 0))\n    \n    # Filter only for tokens that appear multiple times and are consistently predicted as entities\n    token_entity_map = {\n        token: entity for token, (entity, count) in token_entity_counter.items()\n        if count >= 3 and entity > 0 and token and len(token) > 2\n    }\n    \n    # Apply consistent labeling\n    result = df.copy()\n    for token, entity in token_entity_map.items():\n        token_mask = (result[\"tokens\"] == token) & (result[\"preds\"] == 0)\n        result.loc[token_mask, \"preds\"] = entity\n        \n    return result\n\ndef process_chunk(chunk_data):\n    \"\"\"Process a chunk of test data through all models\"\"\"\n    chunk_results = []\n    \n    # Run inference with each model configuration\n    for model_cfg in model_params:\n        chunk_df = inference(model_cfg, chunk_data)\n        chunk_results.append(chunk_df)\n    \n    # Combine results from all models\n    combined_df = pd.concat(chunk_results)\n    \n    # Apply weighted aggregation\n    agg_df = weighted_aggregation(combined_df, model_params)\n    \n    # Get the predicted class (argmax)\n    agg_df[\"preds\"] = np.argmax(\n        np.column_stack([agg_df[f\"prob_{i}\"] for i in range(8)]), \n        axis=1\n    )\n    \n    # Add token texts from the input data\n    token_map = {}\n    for record in chunk_data:\n        for i, token in enumerate(record[\"tokens\"]):\n            token_map[(record[\"document\"], i)] = token\n    \n    agg_df[\"tokens\"] = agg_df.apply(\n        lambda row: token_map.get((row[\"document\"], row[\"word_ids\"]), \"\"), \n        axis=1\n    )\n    \n    # Apply regex rules\n    agg_df = apply_regex_rules(agg_df)\n    \n    # Apply context-aware rules\n    agg_df = context_aware_processing(agg_df)\n    \n    # Apply consistency checks\n    agg_df = token_consistency_check(agg_df)\n    \n    return agg_df\n\ndef convert_to_bio_tags(pred_df):\n    \"\"\"Convert numerical predictions to BIO tagging format\"\"\"\n    df = pred_df.copy()\n    \n    # Convert numeric predictions to BIO tags\n    df[\"bio_tag\"] = df[\"preds\"].map(entity_mapping)\n    \n    # Ensure consecutive predictions of the same entity are properly formatted with B- and I- prefixes\n    df[\"prev_pred\"] = df.groupby(\"document\")[\"preds\"].shift(1).fillna(-1)\n    \n    # If current and previous prediction are the same entity, use I- prefix for second and subsequent tokens\n    for entity_id in range(1, 8):\n        curr_entity_mask = df[\"preds\"] == entity_id\n        prev_same_entity = df[\"prev_pred\"] == entity_id\n        df.loc[curr_entity_mask & prev_same_entity, \"bio_tag\"] = df.loc[curr_entity_mask & prev_same_entity, \"bio_tag\"].str.replace(\"B-\", \"I-\")\n    \n    return df\n\ndef confidence_based_filtering(df, threshold=0.7):\n    \"\"\"Apply confidence-based filtering to reduce false positives\"\"\"\n    result = df.copy()\n    \n    # Calculate top class probability for each token\n    top_probs = np.array([result[f\"prob_{i}\"] for i in range(8)]).max(axis=0)\n    \n    # Filter low-confidence predictions back to \"O\" (non-entity)\n    low_conf_mask = (top_probs < threshold) & (result[\"preds\"] > 0)\n    result.loc[low_conf_mask, \"preds\"] = 0\n    result.loc[low_conf_mask, \"bio_tag\"] = \"O\"\n    \n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T01:19:06.019259Z","iopub.execute_input":"2025-05-12T01:19:06.019499Z","iopub.status.idle":"2025-05-12T01:19:06.041128Z","shell.execute_reply.started":"2025-05-12T01:19:06.019479Z","shell.execute_reply":"2025-05-12T01:19:06.040254Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def main():\n    \"\"\"Main execution function\"\"\"\n    print(\"Loading test data...\")\n    with open(test_data_file, 'r') as f:\n        test_data = json.load(f)\n    \n    print(f\"Processing {len(test_data)} records...\")\n    \n    # Process data in chunks\n    all_results = []\n    for i, chunk_data in enumerate(chunk(test_data, chunk_size)):\n        print(f\"Processing chunk {i+1}/{len(test_data)//chunk_size + 1}\")\n        chunk_results = process_chunk(chunk_data)\n        all_results.append(chunk_results)\n    \n    # Combine all chunk results\n    print(\"Combining results...\")\n    final_df = pd.concat(all_results)\n    \n    # Convert to BIO tagging format\n    print(\"Converting to BIO tags...\")\n    final_df = convert_to_bio_tags(final_df)\n    \n    # Apply confidence-based filtering\n    print(\"Applying confidence filtering...\")\n    final_df = confidence_based_filtering(final_df)\n    \n    # Create submission file\n    print(\"Creating submission...\")\n    \n    # Create submission dataframe\n    submission_rows = []\n    row_id = 0\n    \n    for doc_id, group in final_df.groupby(\"document\"):\n        # Sort by word_ids\n        group = group.sort_values(\"word_ids\")\n        # Skip special tokens (word_ids < 0)\n        valid_tokens = group[group[\"word_ids\"] >= 0]\n        \n        # Add rows for tokens with entity labels (non-O tags)\n        for _, row in valid_tokens[valid_tokens[\"bio_tag\"] != \"O\"].iterrows():\n            submission_rows.append({\n                \"row_id\": row_id,\n                \"document\": int(row[\"document\"]),\n                \"token\": int(row[\"word_ids\"]),\n                \"label\": row[\"bio_tag\"]\n            })\n            row_id += 1\n    \n    # Create DataFrame and save as CSV\n    submission_df = pd.DataFrame(submission_rows)\n    submission_df.to_csv(\"submission.csv\", index=False)\n    \n    print(f\"Done! Submission contains {len(submission_df)} PII detections.\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T01:19:06.041999Z","iopub.execute_input":"2025-05-12T01:19:06.042262Z","iopub.status.idle":"2025-05-12T01:21:01.470616Z","shell.execute_reply.started":"2025-05-12T01:19:06.042244Z","shell.execute_reply":"2025-05-12T01:21:01.469586Z"}},"outputs":[{"name":"stdout","text":"Loading test data...\nProcessing 10 records...\nProcessing chunk 1/1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d2cb0eb5ee04c819fb20bcea231f587"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"673c47cc936e481d8203c20193b83648"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50f08e02ec954369a61a295563587d38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c86de1afe75744089ebea46a8bb79594"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52817f4dd2c84ee18096c221098fc612"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94cfcf7cbb074899aa3d7ffde16bb644"}},"metadata":{}},{"name":"stdout","text":"Combining results...\nConverting to BIO tags...\nApplying confidence filtering...\nCreating submission...\nDone! Submission contains 6947 PII detections.\n","output_type":"stream"}],"execution_count":6}]}