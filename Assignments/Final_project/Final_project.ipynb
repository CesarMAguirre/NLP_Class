{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T01:18:59.220220Z",
     "iopub.status.busy": "2025-05-12T01:18:59.219913Z",
     "iopub.status.idle": "2025-05-12T01:19:05.971681Z",
     "shell.execute_reply": "2025-05-12T01:19:05.970740Z",
     "shell.execute_reply.started": "2025-05-12T01:18:59.220184Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "# Path and configuration settings\n",
    "test_data_file = \"/kaggle/input/pii-detection-removal-from-educational-data/test.json\"\n",
    "weights_path = \"/kaggle/input/piid-modelweights/\"\n",
    "chunk_size = 5_000\n",
    "tokenizer_stride = 32\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Enhanced model parameters with weights\n",
    "model_params = [\n",
    "    {\"path\":\"model_387\", \"max_tokens\":512, \"weight\": 0.8, \"specialization\": \"precision\"},\n",
    "    {\"path\":\"model_539\", \"max_tokens\":1024, \"weight\": 1.0, \"specialization\": \"balanced\"},\n",
    "    {\"path\":\"model_543\", \"max_tokens\":1024, \"weight\": 1.2, \"specialization\": \"balanced\"},\n",
    "    {\"path\":\"model_560\", \"max_tokens\":2048, \"weight\": 1.5, \"specialization\": \"recall\"},\n",
    "    {\"path\":\"model_563\", \"max_tokens\":2048, \"weight\": 1.5, \"specialization\": \"recall\"},\n",
    "    {\"path\":\"model_572\", \"max_tokens\":1024, \"weight\": 1.0, \"specialization\": \"balanced\"},\n",
    "]\n",
    "\n",
    "# Global token cache for efficient processing\n",
    "token_prediction_cache ={}\n",
    "\n",
    "# Entity mapping for NER tagging\n",
    "entity_mapping = {\n",
    "    0: \"O\",\n",
    "    1: \"B-NAME_STUDENT\",\n",
    "    2: \"B-URL_PERSONAL\",\n",
    "    3: \"B-ID_NUM\",\n",
    "    4: \"B-EMAIL\",\n",
    "    5: \"B-STREET_ADDRESS\",\n",
    "    6: \"B-PHONE_NUM\",\n",
    "    7: \"B-USERNAME\",\n",
    "    101: \"I-NAME_STUDENT\",\n",
    "    102: \"I-URL_PERSONAL\",\n",
    "    103: \"I-ID_NUM\",\n",
    "    104: \"I-EMAIL\",\n",
    "    105: \"I-STREET_ADDRESS\",\n",
    "    106: \"I-PHONE_NUM\",\n",
    "    107: \"I-USERNAME\",\n",
    "}\n",
    "# Regex for entity detection\n",
    "regex_patterns = {\n",
    "    \"email\": r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\",\n",
    "    \"phone\": r\"^\\+?[\\d\\-\\(\\)\\s]{7,15}$\",\n",
    "    \"id_number\": r\"^[A-Z0-9]{5,12}$|^\\d{3}-\\d{2}-\\d{4}$\",\n",
    "    \"username\": r\"^@[a-zA-Z0-9_]{3,15}$|^[a-zA-Z][a-zA-Z0-9_]{2,15}$\",\n",
    "    \"url\": r\"^(https?:\\/\\/)?(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)$\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T01:19:05.973425Z",
     "iopub.status.busy": "2025-05-12T01:19:05.973043Z",
     "iopub.status.idle": "2025-05-12T01:19:05.984362Z",
     "shell.execute_reply": "2025-05-12T01:19:05.983377Z",
     "shell.execute_reply.started": "2025-05-12T01:19:05.973403Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Let's define some functions \n",
    "def chunk(l, n): \n",
    "    \"\"\"Split list into chunks of size n\"\"\"\n",
    "    for i in range(0, len(l), n):  \n",
    "        yield l[i:i + n] \n",
    "\n",
    "def decode_targets(targets: list, doc_ids: list) -> list:\n",
    "    \"\"\"Convert numeric targets to BIO tagging format\"\"\"\n",
    "    df = pd.DataFrame({\n",
    "        \"target\": targets,\n",
    "        \"document\": doc_ids\n",
    "    })\n",
    "    # Mark consecutive tokens of same entity as Inside (I-)\n",
    "    df[\"prev_target\"] = df.groupby(\"document\")[\"target\"].shift(1).values\n",
    "    cond = (df[\"prev_target\"] == df[\"target\"]) & (~df[\"prev_target\"].isnull())\n",
    "    df[\"target\"] += 100*cond.astype(int)\n",
    "    \n",
    "    # Map numeric targets to NER tags\n",
    "    df[\"target\"] = df[\"target\"].map(entity_mapping)\n",
    "    \n",
    "    return df[\"target\"].values.tolist()\n",
    "\n",
    "def extract_character_features(tokens):\n",
    "    \"\"\"Extract character-level features from tokens\"\"\"\n",
    "    features = []\n",
    "    for token in tokens:\n",
    "        if not token:  # Handle empty tokens\n",
    "            features.append({\n",
    "                \"length\": 0,\n",
    "                \"has_digits\": False,\n",
    "                \"has_uppercase\": False,\n",
    "                \"has_lowercase\": False,\n",
    "                \"has_special\": False,\n",
    "                \"digit_ratio\": 0,\n",
    "                \"uppercase_ratio\": 0,\n",
    "                \"starts_uppercase\": False,\n",
    "                \"has_email_char\": False,\n",
    "                \"has_url_char\": False\n",
    "            })\n",
    "            continue\n",
    "            \n",
    "        features.append({\n",
    "            \"length\": len(token),\n",
    "            \"has_digits\": any(c.isdigit() for c in token),\n",
    "            \"has_uppercase\": any(c.isupper() for c in token),\n",
    "            \"has_lowercase\": any(c.islower() for c in token),\n",
    "            \"has_special\": any(not c.isalnum() for c in token),\n",
    "            \"digit_ratio\": sum(c.isdigit() for c in token) / max(len(token), 1),\n",
    "            \"uppercase_ratio\": sum(c.isupper() for c in token) / max(len(token), 1),\n",
    "            \"starts_uppercase\": token[0].isupper() if token else False,\n",
    "            \"has_email_char\": \"@\" in token,\n",
    "            \"has_url_char\": \"/\" in token or \".\" in token\n",
    "        })\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T01:19:05.986450Z",
     "iopub.status.busy": "2025-05-12T01:19:05.986202Z",
     "iopub.status.idle": "2025-05-12T01:19:06.001556Z",
     "shell.execute_reply": "2025-05-12T01:19:06.000837Z",
     "shell.execute_reply.started": "2025-05-12T01:19:05.986431Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Data Processing \n",
    "\n",
    "class ListDataset(Dataset):\n",
    "    \"\"\"Simple dataset from a list of items\"\"\"\n",
    "    def __init__(self, data_list):\n",
    "        self.data = data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "def tokenize_and_batch_record(record, tokenizer, max_n_tokens, stride):\n",
    "    \"\"\"Process a single record for transformer input\"\"\"\n",
    "    max_len = min(tokenizer.model_max_length, max_n_tokens)\n",
    "    tokenized_inputs = tokenizer(\n",
    "        record[\"tokens\"], \n",
    "        return_offsets_mapping=False,\n",
    "        verbose=False, \n",
    "        is_split_into_words=True, \n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len, \n",
    "        stride=stride, \n",
    "        truncation=True, \n",
    "        return_overflowing_tokens=True\n",
    "    )\n",
    "\n",
    "    # Extract character-level features\n",
    "    char_features = extract_character_features(record[\"tokens\"])\n",
    "    \n",
    "    # Create a targets map if labels exist\n",
    "    if \"labels\" in record:\n",
    "        # Note: encode_targets function is assumed to be defined elsewhere\n",
    "        targets_map = {i:v for i,v in enumerate(encode_targets(record[\"labels\"]))}\n",
    "    else:\n",
    "        targets_map = {}\n",
    "\n",
    "    # Create batches from tokenized inputs\n",
    "    batch = [{\n",
    "        \"input_ids\": tokenized_inputs[\"input_ids\"][i],\n",
    "        \"attention_mask\": tokenized_inputs[\"attention_mask\"][i],\n",
    "        \"word_ids\": [-100 if x is None else x for x in tokenized_inputs.word_ids(i)],\n",
    "        \"targets\": [targets_map.get(x, -100) for x in tokenized_inputs.word_ids(i)],\n",
    "        \"document\": [record[\"document\"]] * len(tokenized_inputs[\"input_ids\"][i]),\n",
    "        # Add token texts for rule-based processing\n",
    "        \"tokens\": [record[\"tokens\"][x] if x is not None and x < len(record[\"tokens\"]) else \"\" \n",
    "                  for x in tokenized_inputs.word_ids(i)],\n",
    "    } for i in range(len(tokenized_inputs[\"input_ids\"]))]\n",
    "\n",
    "    return batch\n",
    "\n",
    "def tokenize_and_batch(sample, tokenizer, max_n_tokens, stride):\n",
    "    \"\"\"Process a batch of records with progress tracking\"\"\"\n",
    "    tokenized_sample = [tokenize_and_batch_record(rec, tokenizer, max_n_tokens, stride) for rec in tqdm(sample)]\n",
    "    tokenized_sample = [x for xs in tokenized_sample for x in xs]\n",
    "    return tokenized_sample\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Combine individual samples into batches with padding\"\"\"\n",
    "    keys = batch[0].keys()\n",
    "    # Special handling for non-tensor data\n",
    "    non_tensor_keys = [\"tokens\"]\n",
    "    tensor_keys = [k for k in keys if k not in non_tensor_keys]\n",
    "    \n",
    "    # Process tensor data\n",
    "    seq = {k:[torch.tensor(x[k]) for x in batch] for k in tensor_keys}\n",
    "    result = {k:torch.nn.utils.rnn.pad_sequence(v, True, 0) for k,v in seq.items()}\n",
    "    \n",
    "    # Process non-tensor data\n",
    "    for k in non_tensor_keys:\n",
    "        result[k] = [item for x in batch for item in x[k]]\n",
    "        \n",
    "    return result\n",
    "    \n",
    "def create_dataloader(test_data, max_n_tokens, tokenizer_stride):\n",
    "    \"\"\"Create a PyTorch DataLoader\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(weights_path + \"tokenizer\")\n",
    "    tokenized_data = tokenize_and_batch(test_data, tokenizer, max_n_tokens, tokenizer_stride)\n",
    "    return DataLoader(\n",
    "        ListDataset(tokenized_data),\n",
    "        batch_size = 4,\n",
    "        num_workers = 2,\n",
    "        pin_memory = True,\n",
    "        shuffle = False,\n",
    "        collate_fn = collate_fn,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T01:19:06.002802Z",
     "iopub.status.busy": "2025-05-12T01:19:06.002531Z",
     "iopub.status.idle": "2025-05-12T01:19:06.018115Z",
     "shell.execute_reply": "2025-05-12T01:19:06.017278Z",
     "shell.execute_reply.started": "2025-05-12T01:19:06.002782Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "class PiiDetectionModel(torch.nn.Module):\n",
    "    \"\"\"Enhanced wrapper around pre-trained token classification model\"\"\"\n",
    "    def __init__(self, load_path, specialization=None):\n",
    "        super().__init__()\n",
    "        self.backbone = AutoModelForTokenClassification.from_pretrained(load_path)\n",
    "        self.specialization = specialization\n",
    "        \n",
    "    def forward(self, d):\n",
    "        \"\"\"Run inference through the model\"\"\"\n",
    "        preds = self.backbone(\n",
    "            input_ids=d[\"input_ids\"], \n",
    "            attention_mask=d[\"attention_mask\"]\n",
    "        )\n",
    "        \n",
    "        # Apply specialization adjustments if needed\n",
    "        logits = preds.logits\n",
    "        if self.specialization == \"precision\":\n",
    "            # Slightly reduce probability of positive predictions to favor precision\n",
    "            logits[:, :, 1:] -= 0.2\n",
    "        elif self.specialization == \"recall\":\n",
    "            # Slightly boost probability of positive predictions to favor recall\n",
    "            logits[:, :, 1:] += 0.1\n",
    "            \n",
    "        return {\"logits\": logits}\n",
    "\n",
    "def add_confidence_metrics(probs):\n",
    "    \"\"\"Calculate confidence metrics for predictions\"\"\"\n",
    "    # Calculate entropy (lower means more confident)\n",
    "    epsilon = 1e-10  # To avoid log(0)\n",
    "    entropy = -np.sum(probs * np.log(probs + epsilon), axis=1)\n",
    "    \n",
    "    # Calculate margin (difference between top two probabilities)\n",
    "    sorted_probs = np.sort(probs, axis=1)\n",
    "    margin = sorted_probs[:, -1] - sorted_probs[:, -2]\n",
    "    \n",
    "    return entropy, margin\n",
    "\n",
    "def results_to_df(predictions, data, model_name):\n",
    "    \"\"\"Convert model outputs to pandas DataFrame\"\"\"\n",
    "    # Apply softmax to get class probabilities\n",
    "    probs = torch.nn.functional.softmax(predictions[\"logits\"], -1)\n",
    "    probs = probs.flatten(0,1).cpu().numpy()\n",
    "    \n",
    "    # Calculate confidence metrics\n",
    "    entropy, margin = add_confidence_metrics(probs)\n",
    "    \n",
    "    # Create DataFrame with probabilities\n",
    "    probs_df = pd.DataFrame(probs, columns=[f\"prob_{i}\" for i in range(8)])\n",
    "    \n",
    "    # Create DataFrame with metadata\n",
    "    res = pd.DataFrame({\n",
    "        \"document\": data[\"document\"].cpu().flatten(),\n",
    "        \"word_ids\": data[\"word_ids\"].cpu().flatten(),\n",
    "        \"targets\": data[\"targets\"].cpu().flatten(),\n",
    "        \"entropy\": entropy,\n",
    "        \"margin\": margin,\n",
    "        \"model_name\": model_name\n",
    "    })\n",
    "    \n",
    "    # Combine metadata and probabilities\n",
    "    res = pd.concat([res, probs_df], axis=1)\n",
    "    return res\n",
    "\n",
    "def inference(model_cfg, test_data):\n",
    "    \"\"\"Run inference with a single model configuration\"\"\"\n",
    "    # Create model instance\n",
    "    model = PiiDetectionModel(\n",
    "        f\"{weights_path}/{model_cfg['path']}\", \n",
    "        specialization=model_cfg.get('specialization', None)\n",
    "    )\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Create data loader\n",
    "    dl = create_dataloader(test_data, model_cfg['max_tokens'], tokenizer_stride)\n",
    "\n",
    "    # Run inference\n",
    "    res_df = pd.DataFrame()\n",
    "    with torch.no_grad():\n",
    "        for data in dl:\n",
    "            # Move data to device\n",
    "            for k in data.keys():\n",
    "                if isinstance(data[k], torch.Tensor):\n",
    "                    data[k] = data[k].to(device)\n",
    "            \n",
    "            # Get predictions\n",
    "            preds = model(data)\n",
    "            \n",
    "            # Convert to DataFrame and add to results\n",
    "            batch_df = results_to_df(preds, data, model_cfg['path'])\n",
    "            res_df = pd.concat([res_df, batch_df])\n",
    "    \n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T01:19:06.019499Z",
     "iopub.status.busy": "2025-05-12T01:19:06.019259Z",
     "iopub.status.idle": "2025-05-12T01:19:06.041128Z",
     "shell.execute_reply": "2025-05-12T01:19:06.040254Z",
     "shell.execute_reply.started": "2025-05-12T01:19:06.019479Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Post processing\n",
    "def weighted_aggregation(preds_df, model_params):\n",
    "    \"\"\"Aggregate predictions with weighted average\"\"\"\n",
    "    # Create weight lookup\n",
    "    weight_dict = {cfg[\"path\"]: cfg[\"weight\"] for cfg in model_params}\n",
    "    total_weight = sum(weight_dict.values())\n",
    "    \n",
    "    # Calculate weighted probabilities\n",
    "    for i in range(8):\n",
    "        preds_df[f\"weighted_prob_{i}\"] = preds_df[f\"prob_{i}\"] * preds_df[\"model_name\"].map(weight_dict)\n",
    "    \n",
    "    # Group by document and word_ids, and calculate weighted average\n",
    "    result = preds_df.groupby([\"document\", \"word_ids\"]).agg(\n",
    "        **{f\"prob_{i}\":(f'weighted_prob_{i}', 'sum') for i in range(8)},\n",
    "        entropy=(\"entropy\", \"mean\"),\n",
    "        margin=(\"margin\", \"mean\")\n",
    "    )\n",
    "    \n",
    "    # Normalize by total weight\n",
    "    for i in range(8):\n",
    "        result[f\"prob_{i}\"] = result[f\"prob_{i}\"] / total_weight\n",
    "    \n",
    "    return result.reset_index()\n",
    "\n",
    "def apply_regex_rules(test_chunk_df):\n",
    "    \"\"\"Apply regex-based rules to improve detection\"\"\"\n",
    "    df = test_chunk_df.copy()\n",
    "    \n",
    "    # Apply email regex\n",
    "    email_mask = df[\"tokens\"].str.match(regex_patterns[\"email\"], na=False)\n",
    "    df.loc[email_mask, \"preds\"] = 4  # Email class\n",
    "    \n",
    "    # Apply phone regex\n",
    "    phone_mask = df[\"tokens\"].str.match(regex_patterns[\"phone\"], na=False)\n",
    "    df.loc[phone_mask, \"preds\"] = 6  # Phone class\n",
    "    \n",
    "    # Apply ID regex\n",
    "    id_mask = df[\"tokens\"].str.match(regex_patterns[\"id_number\"], na=False)\n",
    "    df.loc[id_mask, \"preds\"] = 3  # ID class\n",
    "    \n",
    "    # Apply username regex\n",
    "    username_mask = df[\"tokens\"].str.match(regex_patterns[\"username\"], na=False)\n",
    "    df.loc[username_mask, \"preds\"] = 7  # Username class\n",
    "    \n",
    "    # Apply URL regex\n",
    "    url_mask = df[\"tokens\"].str.match(regex_patterns[\"url\"], na=False)\n",
    "    df.loc[url_mask, \"preds\"] = 2  # URL class\n",
    "    \n",
    "    return df\n",
    "\n",
    "def context_aware_processing(df):\n",
    "    \"\"\"Apply rules that consider token context\"\"\"\n",
    "    result = df.copy()\n",
    "    \n",
    "    # Create columns for previous and next tokens\n",
    "    result[\"prev_token\"] = result.groupby(\"document\")[\"tokens\"].shift(1)\n",
    "    result[\"next_token\"] = result.groupby(\"document\")[\"tokens\"].shift(-1)\n",
    "    \n",
    "    # Rule 1: Improve name detection with common name parts\n",
    "    common_titles = [\"Mr\", \"Mrs\", \"Ms\", \"Dr\", \"Prof\"]\n",
    "    is_after_title = result[\"prev_token\"].isin(common_titles)\n",
    "    is_capitalized = result[\"tokens\"].str.istitle()\n",
    "    likely_name = is_after_title & is_capitalized & (result[\"preds\"] == 0)\n",
    "    result.loc[likely_name, \"preds\"] = 1  # Mark as name\n",
    "    \n",
    "    # Rule 2: Improve address detection\n",
    "    address_indicators = [\"St\", \"Ave\", \"Rd\", \"Blvd\", \"Ln\", \"Dr\", \"Cir\", \"Apt\"]\n",
    "    preceded_by_number = result[\"prev_token\"].str.match(r\"^\\d+$\", na=False)\n",
    "    followed_by_address_indicator = result[\"next_token\"].isin(address_indicators)\n",
    "    likely_street = preceded_by_number & followed_by_address_indicator\n",
    "    result.loc[likely_street, \"preds\"] = 5  # Mark as address\n",
    "    \n",
    "    # Rule 3: Mark newlines as address components\n",
    "    result.loc[result[\"tokens\"] == \"\\n\", \"preds\"] = 5\n",
    "    \n",
    "    # Rule 4: Clean up predictions - names should start with capital letter\n",
    "    result.loc[(result[\"preds\"] == 1) & (~result[\"tokens\"].str.istitle()), \"preds\"] = 0\n",
    "    \n",
    "    # Rule 5: Consecutive tokens with same entity should be marked consistently\n",
    "    # Propagate entity labels to neighboring tokens if they look like part of the same entity\n",
    "    for entity_id in [1, 2, 3, 4, 5, 6, 7]:  # All entity types\n",
    "        entity_mask = result[\"preds\"] == entity_id\n",
    "        \n",
    "        # Forward propagation\n",
    "        for _ in range(2):  # Apply twice to handle longer entities\n",
    "            next_entity_mask = result.groupby(\"document\")[\"preds\"].shift(-1) == entity_id\n",
    "            propagate_forward = (~entity_mask) & next_entity_mask & result[\"tokens\"].notna()\n",
    "            result.loc[propagate_forward, \"preds\"] = entity_id + 100  # Mark as I-entity\n",
    "        \n",
    "        # Backward propagation\n",
    "        for _ in range(2):\n",
    "            prev_entity_mask = result.groupby(\"document\")[\"preds\"].shift(1) == entity_id\n",
    "            propagate_backward = (~entity_mask) & prev_entity_mask & result[\"tokens\"].notna()\n",
    "            result.loc[propagate_backward, \"preds\"] = entity_id + 100  # Mark as I-entity\n",
    "    \n",
    "    return result\n",
    "\n",
    "def token_consistency_check(df):\n",
    "    \"\"\"Apply consistency checks across documents\"\"\"\n",
    "    # Create a mapping of commonly detected tokens to their most frequent entity\n",
    "    token_entity_counter = df.groupby(\"tokens\")[\"preds\"].apply(lambda x: Counter(x).most_common(1)[0] if len(x) > 0 else (0, 0))\n",
    "    \n",
    "    # Filter only for tokens that appear multiple times and are consistently predicted as entities\n",
    "    token_entity_map = {\n",
    "        token: entity for token, (entity, count) in token_entity_counter.items()\n",
    "        if count >= 3 and entity > 0 and token and len(token) > 2\n",
    "    }\n",
    "    \n",
    "    # Apply consistent labeling\n",
    "    result = df.copy()\n",
    "    for token, entity in token_entity_map.items():\n",
    "        token_mask = (result[\"tokens\"] == token) & (result[\"preds\"] == 0)\n",
    "        result.loc[token_mask, \"preds\"] = entity\n",
    "        \n",
    "    return result\n",
    "\n",
    "def process_chunk(chunk_data):\n",
    "    \"\"\"Process a chunk of test data through all models\"\"\"\n",
    "    chunk_results = []\n",
    "    \n",
    "    # Run inference with each model configuration\n",
    "    for model_cfg in model_params:\n",
    "        chunk_df = inference(model_cfg, chunk_data)\n",
    "        chunk_results.append(chunk_df)\n",
    "    \n",
    "    # Combine results from all models\n",
    "    combined_df = pd.concat(chunk_results)\n",
    "    \n",
    "    # Apply weighted aggregation\n",
    "    agg_df = weighted_aggregation(combined_df, model_params)\n",
    "    \n",
    "    # Get the predicted class (argmax)\n",
    "    agg_df[\"preds\"] = np.argmax(\n",
    "        np.column_stack([agg_df[f\"prob_{i}\"] for i in range(8)]), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Add token texts from the input data\n",
    "    token_map = {}\n",
    "    for record in chunk_data:\n",
    "        for i, token in enumerate(record[\"tokens\"]):\n",
    "            token_map[(record[\"document\"], i)] = token\n",
    "    \n",
    "    agg_df[\"tokens\"] = agg_df.apply(\n",
    "        lambda row: token_map.get((row[\"document\"], row[\"word_ids\"]), \"\"), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Apply regex rules\n",
    "    agg_df = apply_regex_rules(agg_df)\n",
    "    \n",
    "    # Apply context-aware rules\n",
    "    agg_df = context_aware_processing(agg_df)\n",
    "    \n",
    "    # Apply consistency checks\n",
    "    agg_df = token_consistency_check(agg_df)\n",
    "    \n",
    "    return agg_df\n",
    "\n",
    "def convert_to_bio_tags(pred_df):\n",
    "    \"\"\"Convert numerical predictions to BIO tagging format\"\"\"\n",
    "    df = pred_df.copy()\n",
    "    \n",
    "    # Convert numeric predictions to BIO tags\n",
    "    df[\"bio_tag\"] = df[\"preds\"].map(entity_mapping)\n",
    "    \n",
    "    # Ensure consecutive predictions of the same entity are properly formatted with B- and I- prefixes\n",
    "    df[\"prev_pred\"] = df.groupby(\"document\")[\"preds\"].shift(1).fillna(-1)\n",
    "    \n",
    "    # If current and previous prediction are the same entity, use I- prefix for second and subsequent tokens\n",
    "    for entity_id in range(1, 8):\n",
    "        curr_entity_mask = df[\"preds\"] == entity_id\n",
    "        prev_same_entity = df[\"prev_pred\"] == entity_id\n",
    "        df.loc[curr_entity_mask & prev_same_entity, \"bio_tag\"] = df.loc[curr_entity_mask & prev_same_entity, \"bio_tag\"].str.replace(\"B-\", \"I-\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def confidence_based_filtering(df, threshold=0.7):\n",
    "    \"\"\"Apply confidence-based filtering to reduce false positives\"\"\"\n",
    "    result = df.copy()\n",
    "    \n",
    "    # Calculate top class probability for each token\n",
    "    top_probs = np.array([result[f\"prob_{i}\"] for i in range(8)]).max(axis=0)\n",
    "    \n",
    "    # Filter low-confidence predictions back to \"O\" (non-entity)\n",
    "    low_conf_mask = (top_probs < threshold) & (result[\"preds\"] > 0)\n",
    "    result.loc[low_conf_mask, \"preds\"] = 0\n",
    "    result.loc[low_conf_mask, \"bio_tag\"] = \"O\"\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T01:19:06.042262Z",
     "iopub.status.busy": "2025-05-12T01:19:06.041999Z",
     "iopub.status.idle": "2025-05-12T01:21:01.470616Z",
     "shell.execute_reply": "2025-05-12T01:21:01.469586Z",
     "shell.execute_reply.started": "2025-05-12T01:19:06.042244Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data...\n",
      "Processing 10 records...\n",
      "Processing chunk 1/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d2cb0eb5ee04c819fb20bcea231f587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "673c47cc936e481d8203c20193b83648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50f08e02ec954369a61a295563587d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c86de1afe75744089ebea46a8bb79594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52817f4dd2c84ee18096c221098fc612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94cfcf7cbb074899aa3d7ffde16bb644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining results...\n",
      "Converting to BIO tags...\n",
      "Applying confidence filtering...\n",
      "Creating submission...\n",
      "Done! Submission contains 6947 PII detections.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"Loading test data...\")\n",
    "    with open(test_data_file, 'r') as f:\n",
    "        test_data = json.load(f)\n",
    "    \n",
    "    print(f\"Processing {len(test_data)} records...\")\n",
    "    \n",
    "    # Process data in chunks\n",
    "    all_results = []\n",
    "    for i, chunk_data in enumerate(chunk(test_data, chunk_size)):\n",
    "        print(f\"Processing chunk {i+1}/{len(test_data)//chunk_size + 1}\")\n",
    "        chunk_results = process_chunk(chunk_data)\n",
    "        all_results.append(chunk_results)\n",
    "    \n",
    "    # Combine all chunk results\n",
    "    print(\"Combining results...\")\n",
    "    final_df = pd.concat(all_results)\n",
    "    \n",
    "    # Convert to BIO tagging format\n",
    "    print(\"Converting to BIO tags...\")\n",
    "    final_df = convert_to_bio_tags(final_df)\n",
    "    \n",
    "    # Apply confidence-based filtering\n",
    "    print(\"Applying confidence filtering...\")\n",
    "    final_df = confidence_based_filtering(final_df)\n",
    "    \n",
    "    # Create submission file\n",
    "    print(\"Creating submission...\")\n",
    "    \n",
    "    # Create submission dataframe\n",
    "    submission_rows = []\n",
    "    row_id = 0\n",
    "    \n",
    "    for doc_id, group in final_df.groupby(\"document\"):\n",
    "        # Sort by word_ids\n",
    "        group = group.sort_values(\"word_ids\")\n",
    "        # Skip special tokens (word_ids < 0)\n",
    "        valid_tokens = group[group[\"word_ids\"] >= 0]\n",
    "        \n",
    "        # Add rows for tokens with entity labels (non-O tags)\n",
    "        for _, row in valid_tokens[valid_tokens[\"bio_tag\"] != \"O\"].iterrows():\n",
    "            submission_rows.append({\n",
    "                \"row_id\": row_id,\n",
    "                \"document\": int(row[\"document\"]),\n",
    "                \"token\": int(row[\"word_ids\"]),\n",
    "                \"label\": row[\"bio_tag\"]\n",
    "            })\n",
    "            row_id += 1\n",
    "    \n",
    "    # Create DataFrame and save as CSV\n",
    "    submission_df = pd.DataFrame(submission_rows)\n",
    "    submission_df.to_csv(\"submission.csv\", index=False)\n",
    "    \n",
    "    print(f\"Done! Submission contains {len(submission_df)} PII detections.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7500999,
     "sourceId": 66653,
     "sourceType": "competition"
    },
    {
     "datasetId": 4904844,
     "sourceId": 8263220,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
